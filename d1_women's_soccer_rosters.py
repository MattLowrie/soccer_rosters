# -*- coding: utf-8 -*-
"""D1 Women's Soccer Rosters.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KfRuomeq8qA1eW__2lKL52J1L5OBswbU
"""
# !pip install -q beautifulsoup4
# !pip install -q google
# !pip install -q requests
# !pip install -q wikitables
# !pip install --upgrade certifi

import codecs
import csv
import logging
import re
import requests
import time
import ncaa_roster_parser
import brotli
from bs4 import BeautifulSoup as bs
from googlesearch import search
from urllib.parse import urlparse
from urllib.parse import urlunparse
from wikitables import import_tables

def main():
  logger = logging.getLogger(__name__)
  logger.setLevel(logging.DEBUG)
  # d1_roster_urls.csv headers:
  roster_headers = ['Institution', 'Location', 'State', 'Type', 'Nickname', 'Conference', 'Url']
  schools = []
  locations = []
  states = []
  types = []
  nicknames = []
  conferences = []
  urls = []
  with open('d1_roster_urls.csv', 'r') as rosters_csv:
    reader = csv.DictReader(rosters_csv)
    for row in reader:
      if row['Institution'] == 'Austin Peay':
        schools.append(row['Institution'])
        locations.append(row['Location'])
        states.append(row['State'])
        types.append(row['Type'])
        nicknames.append(row['Nickname'])
        conferences.append(row['Conference'])
        urls.append(row['Url'])

  print('\n'.join(urls))

  http_headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.110 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'en-US,en;q=0.9'
  }

  request_args = []
  corrected_urls = []
  for i, url in enumerate(urls):
    parsed_url = urlparse(url)
    params = {}
    if parsed_url.query:
      query_params = parsed_url.query.split('&')
      for query_param in query_params:
        if '=' in query_param:
          k, v = query_param.split('=')
          # SidearmSports sites use the path= query param to designate the
          # sport to display. For women's soccer, the value of this param can
          # be wsoc, wsoccer, soc, or <empty string>. Since the Google results
          # can potentially provide the link to the wrong sport, we verify that
          # the path= value will pull up the women's soccer roster.
          if k == 'path' and v:
            # If the param is 'path' and the value is NOT empty string ...
            if 'soc' not in v:
              # ... and the value doesn't contain 'soc', set the param value to
              # which should work in all cases.
              params[k] = 'wsoc'
          else:
            params[k] = v
    request_args.append({
        'url': urlunparse((
            parsed_url.scheme,
            parsed_url.netloc,
            parsed_url.path,
            None, None, None)),
        'params': params,
        'headers': http_headers,
    })
    corrected_urls.append(request_args[i]['url'])
  # index_start = 220
  # index_end = 230
  # test_set = request_args[index_start:index_end]
  # for tester in test_set:
  #   print(tester['url'])

  soups = []
  for req_args in request_args:
  # for req_args in test_set:
    try:
      r = requests.get(**req_args)
      if r.status_code == 200:
        if ('Content-encoding' in r.headers and
            r.headers['Content-Encoding'] == 'br'):
          soups.append(bs(brotli.decompress(r.content), 'html.parser'))
        else:
          soups.append(bs(r.content, 'html.parser'))
      else:
        soups.append('')
        logger.warn('Status code %d for %s', r.status_code, req_args['url'])
    except requests.exceptions.ConnectionError:
      logger.error('Connection error for: %s', req_args['url'])
      soups.append('')


  teams = []
  for i, soup in enumerate(soups):
    logger.debug('=' * 20)
    logger.debug(urls[i])
    if 'roster.aspx' in urls[i] and soup:
      sidearm = ncaa_roster_parser.SidearmProcessor(soup)
      teams.append(sidearm.GetTeam())
    elif '2018-19/roster' in urls[i] and soup:
      table_proc = ncaa_roster_parser.TableProcessor(soup)
      teams.append(table_proc.GetTeam())
    else:
      logger.warn('Site processor not found: %s', urls[i])
      teams.append({})

  csv_rows = []
  for i, team in enumerate(teams):
  # for offset, team in enumerate(teams):
    # i = index_start + offset
    for player in team:
      csv_row = ','.join([
        schools[i],
        locations[i],
        states[i],
        types[i],
        nicknames[i],
        conferences[i],
        urls[i],
      ])
      if isinstance(player, str):
        csv_row =+ player
      elif isinstance(player, dict):
        for k in player.keys():
          csv_row += ',' + player[k]
      csv_rows.append(csv_row)

  with codecs.open('rosters-austinpeay.csv', 'w', 'utf-8-sig') as fw:
    for csv_row in csv_rows:
      fw.write(csv_row + '\n')
  # players = soup.findAll('li', attrs={'class': 'sidearm-roster-player'})
  # print(len(players))
  # for player in players:
  #   name_text = player.select('[class*="name"]')[0].get_text()
  #   match = re.search(NAME_RE, name_text)
  #   if match:
  #     name = match.group()
  #   else:
  #     name = name_text.strip()
  #   print(name)
  # print(players[0].select('[class*="name"]')[0].get_text().strip())
  # print(players[0].select('[class*="position"]')[0].get_text().strip())
  # print(players[0].select('[class*="height"]')[0].get_text().strip())
  # infos = players[0].get_text().split('\n')
  # for info in infos:
  #  sinfo = info.strip()
  #  if sinfo:
  #    print(sinfo)
  # garbage = soup.get_text().split('\n')
  # print(garbage.index('Players'))
  # i = garbage.index('Players')
  # clean = []
  # for g in garbage[i:]:
  #  if g.strip():
  #    clean.append(g.strip())
  # print('\n'.join(clean))

if __name__ == '__main__':
  fmt = '%(asctime)s,%(msecs)-3d %(levelname)-8s %(filename)s - %(message)s'
  f = logging.Formatter(fmt=fmt, datefmt='%m-%d %H:%M:%S')
  h = logging.FileHandler('/tmp/d1_rosters.log', 'w', encoding='UTF-8')
  h.setFormatter(f)
  h.setLevel(logging.DEBUG)
  logging.basicConfig(handlers=[h])
  main()
